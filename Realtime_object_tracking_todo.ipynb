{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Realtime_object_tracking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/akwasnie/SiMa-GUT/blob/main/Realtime_object_tracking_todo.ipynb)"
      ],
      "metadata": {
        "id": "qe9YMGGYa9NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xX3kUxdra7Ft"
      },
      "outputs": [],
      "source": [
        "# In Jupyter, you would need to install TF 2 via !pip.\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "DV7RDA4r5gEl",
        "outputId": "c6f7740b-44b5-4999-8c86-09eb6609b37f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import cv2\n",
        "import IPython\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import torch\n",
        "from google.colab import output\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image, Javascript, clear_output, display\n",
        "from numpy import asarray\n",
        "from PIL import Image as pimage"
      ],
      "metadata": {
        "id": "nRHtTnEl5Ae2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/akwasnie/CenterNet.git && cd CenterNet && git checkout 8ef87b433529ac8f8bd4f95707f6bc05052c55e9"
      ],
      "metadata": {
        "id": "6MNuo7X05RCu",
        "outputId": "7ab95dc5-38fd-4845-c203-49750eb4a06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CenterNet'...\n",
            "remote: Enumerating objects: 391, done.\u001b[K\n",
            "remote: Total 391 (delta 0), reused 0 (delta 0), pack-reused 391\n",
            "Receiving objects: 100% (391/391), 6.26 MiB | 10.90 MiB/s, done.\n",
            "Resolving deltas: 100% (174/174), done.\n",
            "Note: checking out '8ef87b433529ac8f8bd4f95707f6bc05052c55e9'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 8ef87b4 Merge pull request #400 from jscsmk/master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('CenterNet')\n",
        "from CenterNet import src\n",
        "from src.lib.utils.image import get_affine_transform\n",
        "from src.lib.models.decode import ctdet_decode\n",
        "from src.lib.utils.post_process import ctdet_post_process"
      ],
      "metadata": {
        "id": "U72ukvG61KoD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'data/'\n",
        "MODEL_FILE = os.path.join(DATA_DIR, 'ctdet_coco_dlav0_1x.onnx')\n",
        "CLASS_FILE = os.path.join(DATA_DIR, 'coco.json')\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "if not os.path.exists(MODEL_FILE):\n",
        "    !gdown --id 1Nda64Ezeo1yObABzDpJNzr-3n1yn_VOa -O $MODEL_FILE\n",
        "else:\n",
        "    print('CSV file ({}) already exists.'.format(MODEL_FILE))\n",
        "\n",
        "if not os.path.exists(CLASS_FILE):\n",
        "    !gdown --id 1ddXo-vbPNvNRs4X-faUkcwBtaWSP7ZtO -O $CLASS_FILE\n",
        "else:\n",
        "    print('CSV file ({}) already exists.'.format(CLASS_FILE))"
      ],
      "metadata": {
        "id": "v4audkP73Odr",
        "outputId": "2a628359-9a6d-4c8e-9408-40ebb8f8e036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Nda64Ezeo1yObABzDpJNzr-3n1yn_VOa\n",
            "To: /content/data/ctdet_coco_dlav0_1x.onnx\n",
            "100% 74.5M/74.5M [00:00<00:00, 79.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ddXo-vbPNvNRs4X-faUkcwBtaWSP7ZtO\n",
            "To: /content/data/coco.json\n",
            "100% 2.92k/2.92k [00:00<00:00, 2.39MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IN_SIZE = [512,512]\n",
        "MEAN = [0.408, 0.447, 0.470]\n",
        "STD = [0.289, 0.274, 0.278]\n",
        "MAX_BB_NUM = 10\n",
        "THRESHOLD = 0.25\n",
        "FILENAME = 'photo.jpg'"
      ],
      "metadata": {
        "id": "SiK1Iz6G5yig"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path):\n",
        "  image = cv2.imread(image_path)\n",
        "  frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  h, w = frame.shape[:2]\n",
        "  # c, s calcualtion \n",
        "  trans_input = get_affine_transform(c[0], s[0], 0, IN_SIZE)\n",
        "  im_data = cv2.warpAffine(\n",
        "      frame, trans_input,\n",
        "      (IN_SIZE[0], IN_SIZE[1]),\n",
        "      flags=cv2.INTER_LINEAR,\n",
        "  )\n",
        "  # normalize (standarization)\n",
        "  im_data = im_data.transpose(2, 0, 1).reshape(1, 3, IN_SIZE[0], IN_SIZE[1])\n",
        "  return frame, im_data, c, s\n",
        "\n",
        "def import_data(model_path, class_mapping_path):\n",
        "  ort_session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "  with open(class_mapping_path, 'r') as fp:\n",
        "    category_index = json.load(fp)\n",
        "  return ort_session, category_index\n",
        "\n",
        "def centernet2cocodict(out, c, m, category_index):\n",
        "    hm = torch.from_numpy(out[0])\n",
        "    wh = torch.from_numpy(out[2])\n",
        "    reg = torch.from_numpy(out[1])\n",
        "    hm = hm.sigmoid_()\n",
        "    # decode detections & postprocess\n",
        "\n",
        "    results = {}  \n",
        "    for j in range(1, 80 + 1):\n",
        "        results[j] = np.array(dets[j], dtype=np.float32).reshape(-1, 5)\n",
        "    \n",
        "    scores = np.hstack([results[j][:, 4] for j in range(1, 80 + 1)])\n",
        "\n",
        "    out_dict = {\n",
        "        'num_detections': MAX_BB_NUM,\n",
        "        'detection_boxes': [],\n",
        "        'detection_scores': [],\n",
        "        'detection_classes': [],\n",
        "        }\n",
        "\n",
        "    cats = list(category_index.values())\n",
        "    cats.sort(key=lambda x: x['id'])\n",
        "\n",
        "    for j in range(1, 80 + 1):\n",
        "        for bbox in results[j]:\n",
        "            if bbox[4] > THRESHOLD:\n",
        "                out_dict['detection_boxes'].append(bbox[:4])\n",
        "                out_dict['detection_scores'].append(bbox[4])\n",
        "                out_dict['detection_classes'].append(cats[j-1]['id'])\n",
        "\n",
        "    out_dict['detection_boxes'] = np.asarray(out_dict['detection_boxes'])\n",
        "    out_dict['detection_scores'] = np.asarray(out_dict['detection_scores'])\n",
        "    out_dict['detection_classes'] = np.asarray(out_dict['detection_classes'])\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "def run_inference(ort_session, category_index, image_data, c, s):\n",
        "  output = ort_session.run(None, {ort_session.get_inputs()[0].name: image_data})\n",
        "  output_dict = centernet2cocodict(output, c, s, category_index)\n",
        "  return output_dict\n",
        "\n",
        "ort_session, category_index = import_data(MODEL_FILE, CLASS_FILE)"
      ],
      "metadata": {
        "id": "UFdHZx5h4ARR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O $FILENAME https://m.media-amazon.com/images/I/71lkKY9oGWL._AC_SX450_.jpg"
      ],
      "metadata": {
        "id": "PtzP0_y_J78x",
        "outputId": "fae6cf1b-5b51-41ab-d823-822792a8f5b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-09 04:02:15--  https://m.media-amazon.com/images/I/71lkKY9oGWL._AC_SX450_.jpg\n",
            "Resolving m.media-amazon.com (m.media-amazon.com)... 54.230.87.225, 2600:9000:2200:ce00:1d:d7f6:39cf:a761, 2600:9000:2200:3800:1d:d7f6:39cf:a761, ...\n",
            "Connecting to m.media-amazon.com (m.media-amazon.com)|54.230.87.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30389 (30K) [image/jpeg]\n",
            "Saving to: ‘photo.jpg’\n",
            "\n",
            "photo.jpg           100%[===================>]  29.68K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-02-09 04:02:16 (8.06 MB/s) - ‘photo.jpg’ saved [30389/30389]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame, image_data, c, s = preprocess(FILENAME)\n",
        "ort_session, category_index = import_data(MODEL_FILE, CLASS_FILE)\n",
        "output_dict = run_inference(ort_session, category_index, image_data, c, s)\n",
        "print(output_dict)\n",
        "for i in range(len(output_dict['detection_boxes'])):\n",
        "  bbox = output_dict['detection_boxes'][i]\n",
        "  class_name = category_index[str(output_dict['detection_classes'][i])]['name']\n",
        "  cv2.rectangle(frame,\n",
        "                (int(bbox[0]), int(bbox[1])),\n",
        "                (int(bbox[2]), int(bbox[3])),\n",
        "                (0,255,0),\n",
        "                2)\n",
        "  cv2.putText(frame,\n",
        "              class_name,\n",
        "              (int(bbox[0]), int(bbox[1]-10)),\n",
        "              0,\n",
        "              0.45,\n",
        "              (255,0,0),\n",
        "              0)\n",
        "result = np.asarray(frame)\n",
        "result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "cv2_imshow(result)"
      ],
      "metadata": {
        "id": "cIUg1hfC957b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as pimage\n",
        "def ndarray_to_b64(ndarray):\n",
        "    # Converts a np ndarray to a b64 string readable by html-img tags. \n",
        "    img = cv2.cvtColor(ndarray, cv2.COLOR_RGB2BGR)\n",
        "    _, buffer = cv2.imencode('.png', img)\n",
        "    return b64encode(buffer).decode('utf-8')\n",
        "\n",
        "def predict(img_64):\n",
        "  binary = b64decode(img_64.split(',')[1])\n",
        "  with open('photo.jpg', 'wb') as f:\n",
        "    f.write(binary)\n",
        "\n",
        "  image = pimage.open('photo.jpg')\n",
        "  data = asarray(image)\n",
        "\n",
        "  result = 'data:image/jpeg;base64,' + ndarray_to_b64(data)\n",
        "  return IPython.display.JSON({'result': result})\n",
        "\n",
        "output.register_callback('amld.predict', predict)"
      ],
      "metadata": {
        "id": "aiMQxD-Ra_NX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=no\">\n",
        "<button id=\"start\">start</button><button id=\"clear\">clear</button><br />\n",
        "<canvas width=\"320\" height=\"180\" id=\"canvas\" style=\"border:1px solid black\"></canvas><br />\n",
        "<video id=\"myVideo\" width=\"320\" height=\"180\"></video><br />\n",
        "<image id=\"image\"></image>\n",
        "<script>\n",
        "  let canvas = document.getElementById('canvas')\n",
        "  let output = document.getElementById('output')\n",
        "  let ctx = canvas.getContext('2d')\n",
        "  let img_64\n",
        "  let dragging = false\n",
        "  let timeout\n",
        "  let stream \n",
        "  let video = document.getElementById('myVideo')\n",
        "\n",
        "\n",
        "  let predict = () => {\n",
        "    google.colab.kernel.invokeFunction('amld.predict', [img_64], {}).then(\n",
        "        obj => document.getElementById('image').src = obj.data['application/json'].result)\n",
        "  }\n",
        "\n",
        "  async function startvideo(){\n",
        "    stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false })\n",
        "    video.srcObject = stream\n",
        "    await video.play();\n",
        "  }\n",
        "  \n",
        "  async function sendforprediction() {\n",
        "    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)\n",
        "   \timg_64 = canvas.toDataURL('image/jpeg', 0.9)\n",
        "    clearTimeout(timeout)\n",
        "    timeout = setTimeout(predict, 500)\n",
        "  }\n",
        "\n",
        "  const handler = e => {\n",
        "    sendforprediction()\n",
        "  }\n",
        "  canvas.addEventListener('touchstart', e => {dragging=true; handler(e)})\n",
        "  canvas.addEventListener('touchmove', e => {e.preventDefault(); dragging && handler(e)})\n",
        "  canvas.addEventListener('touchend', () => dragging=false)\n",
        "  canvas.addEventListener('mousedown', e => {dragging=true; handler(e)})\n",
        "  canvas.addEventListener('mousemove', e => {dragging && handler(e)})\n",
        "  canvas.addEventListener('mouseup', () => dragging=false)\n",
        "  canvas.addEventListener('mouseleave', () => dragging=false)\n",
        "  document.getElementById('clear').addEventListener('click', () => {\n",
        "    ctx.fillStyle = 'white'\n",
        "    ctx.fillRect(0, 0, 320, 180)\n",
        "    stream.getTracks().forEach(function(track) {\n",
        "        track.stop();\n",
        "    });\n",
        "    clear_output();\n",
        "    video.srcObject = null;\n",
        "  })\n",
        "  document.getElementById('start').addEventListener('click', () => {\n",
        "    startvideo()\n",
        "  })\n",
        "</script>"
      ],
      "metadata": {
        "id": "uAaoeWhIbOgh",
        "outputId": "900004b7-c174-4d6e-ca78-0243a815bbee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=no\">\n",
              "<button id=\"start\">start</button><button id=\"clear\">clear</button><br />\n",
              "<canvas width=\"320\" height=\"180\" id=\"canvas\" style=\"border:1px solid black\"></canvas><br />\n",
              "<video id=\"myVideo\" width=\"320\" height=\"180\"></video><br />\n",
              "<image id=\"image\"></image>\n",
              "<script>\n",
              "  let canvas = document.getElementById('canvas')\n",
              "  let output = document.getElementById('output')\n",
              "  let ctx = canvas.getContext('2d')\n",
              "  let img_64\n",
              "  let dragging = false\n",
              "  let timeout\n",
              "  let stream \n",
              "  let video = document.getElementById('myVideo')\n",
              "\n",
              "\n",
              "  let predict = () => {\n",
              "    google.colab.kernel.invokeFunction('amld.predict', [img_64], {}).then(\n",
              "        obj => document.getElementById('image').src = obj.data['application/json'].result)\n",
              "  }\n",
              "\n",
              "  async function startvideo(){\n",
              "    stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false })\n",
              "    video.srcObject = stream\n",
              "    await video.play();\n",
              "  }\n",
              "  \n",
              "  async function sendforprediction() {\n",
              "    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)\n",
              "   \timg_64 = canvas.toDataURL('image/jpeg', 0.9)\n",
              "    clearTimeout(timeout)\n",
              "    timeout = setTimeout(predict, 500)\n",
              "  }\n",
              "\n",
              "  const handler = e => {\n",
              "    sendforprediction()\n",
              "  }\n",
              "  canvas.addEventListener('touchstart', e => {dragging=true; handler(e)})\n",
              "  canvas.addEventListener('touchmove', e => {e.preventDefault(); dragging && handler(e)})\n",
              "  canvas.addEventListener('touchend', () => dragging=false)\n",
              "  canvas.addEventListener('mousedown', e => {dragging=true; handler(e)})\n",
              "  canvas.addEventListener('mousemove', e => {dragging && handler(e)})\n",
              "  canvas.addEventListener('mouseup', () => dragging=false)\n",
              "  canvas.addEventListener('mouseleave', () => dragging=false)\n",
              "  document.getElementById('clear').addEventListener('click', () => {\n",
              "    ctx.fillStyle = 'white'\n",
              "    ctx.fillRect(0, 0, 320, 180)\n",
              "    stream.getTracks().forEach(function(track) {\n",
              "        track.stop();\n",
              "    });\n",
              "    clear_output();\n",
              "    video.srcObject = null;\n",
              "  })\n",
              "  document.getElementById('start').addEventListener('click', () => {\n",
              "    startvideo()\n",
              "  })\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EYlTtRCNn0e2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}